{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-11T21:40:01.016033Z",
     "start_time": "2025-08-11T21:39:59.246257Z"
    }
   },
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Импорт и пути\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple\n",
    "from collections import Counter\n",
    "import json\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "BASE_DIR = Path(\"\")\n",
    "TX_PATH = BASE_DIR / \"transaction_fraud_data.parquet\"\n",
    "FX_PATH = BASE_DIR / \"historical_currency_exchange.parquet\"\n",
    "OUT_DIR = BASE_DIR / \"eda_outputs\"\n",
    "PLOTS_DIR = OUT_DIR / \"plots\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PLOTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"TX_PATH:\", TX_PATH)\n",
    "print(\"FX_PATH:\", FX_PATH)\n",
    "print(\"OUT_DIR:\", OUT_DIR)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TX_PATH: transaction_fraud_data.parquet\n",
      "FX_PATH: historical_currency_exchange.parquet\n",
      "OUT_DIR: eda_outputs\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T21:40:01.031690Z",
     "start_time": "2025-08-11T21:40:01.019622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Утилиты: проверки, нормализация, разворот вложенных полей\n",
    "\n",
    "def ensure_files_exist():\n",
    "    missing = [p.name for p in [TX_PATH, FX_PATH] if not p.exists()]\n",
    "    if missing:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Отсутствуют файлы: {', '.join(missing)}. \"\n",
    "            \"Проверь пути в TX_PATH/FX_PATH.\"\n",
    "        )\n",
    "\n",
    "def normalize_timestamp(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\", utc=True)\n",
    "    df[\"timestamp\"] = df[\"timestamp\"].dt.tz_convert(None)\n",
    "    return df\n",
    "\n",
    "def expand_last_hour_activity(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if \"last_hour_activity\" in df.columns:\n",
    "        expanded = pd.json_normalize(df[\"last_hour_activity\"].dropna())\n",
    "        for col in [\"num_transactions\", \"total_amount\", \"unique_merchants\", \"unique_countries\", \"max_single_amount\"]:\n",
    "            if col not in expanded.columns:\n",
    "                expanded[col] = np.nan\n",
    "        expanded = expanded.reindex(df.index)\n",
    "        expanded.columns = [f\"lha_{c}\" for c in expanded.columns]\n",
    "        df = pd.concat([df.drop(columns=[\"last_hour_activity\"]), expanded], axis=1)\n",
    "    return df\n",
    "\n",
    "def basic_qc(df: pd.DataFrame) -> Dict[str, int]:\n",
    "    info = {}\n",
    "    if \"transaction_id\" in df.columns:\n",
    "        info[\"duplicate_transaction_id\"] = int(df[\"transaction_id\"].duplicated().sum())\n",
    "    info[\"rows\"] = int(len(df))\n",
    "    info[\"na_timestamp\"] = int(df[\"timestamp\"].isna().sum()) if \"timestamp\" in df.columns else None\n",
    "    info[\"na_amount\"] = int(df[\"amount\"].isna().sum()) if \"amount\" in df.columns else None\n",
    "    return info\n"
   ],
   "id": "d1f04c63e261abad",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T21:40:01.408036Z",
     "start_time": "2025-08-11T21:40:01.391925Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Валюты: подготовка FX и конвертация сумм в USD\n",
    "\n",
    "def melt_fx_long(fx: pd.DataFrame) -> pd.DataFrame:\n",
    "    fx = fx.copy()\n",
    "    fx[\"date\"] = pd.to_datetime(fx[\"date\"]).dt.normalize()\n",
    "    long = fx.melt(id_vars=[\"date\"], var_name=\"currency\", value_name=\"rate\")\n",
    "    long[\"rate\"] = pd.to_numeric(long[\"rate\"], errors=\"coerce\")\n",
    "    long = long.sort_values([\"currency\", \"date\"])\n",
    "    return long\n",
    "\n",
    "def convert_to_usd(df: pd.DataFrame, fx: pd.DataFrame, tolerance_days: int = 7):\n",
    "    df = df.copy()\n",
    "\n",
    "    ts = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "    df[\"timestamp\"] = ts\n",
    "    df[\"date\"] = ts.dt.normalize().astype(\"datetime64[ns]\")\n",
    "    df[\"amount\"] = pd.to_numeric(df[\"amount\"], errors=\"coerce\")\n",
    "    df[\"currency\"] = df[\"currency\"].astype(\"string\")\n",
    "    df[\"_orig_idx\"] = np.arange(len(df))\n",
    "\n",
    "    fx = fx.copy()\n",
    "    fx[\"date\"] = pd.to_datetime(fx[\"date\"], errors=\"coerce\").dt.normalize().astype(\"datetime64[ns]\")\n",
    "    fx_long = fx.melt(id_vars=[\"date\"], var_name=\"currency\", value_name=\"rate\")\n",
    "    fx_long[\"currency\"] = fx_long[\"currency\"].astype(\"string\")\n",
    "    fx_long[\"rate\"] = pd.to_numeric(fx_long[\"rate\"], errors=\"coerce\")\n",
    "\n",
    "    known_ccy = set(fx_long[\"currency\"].dropna().unique().tolist())\n",
    "    df_ccy = df[\"currency\"].dropna().unique().tolist()\n",
    "    present_ccy = set([str(c) for c in df_ccy if c is not None])\n",
    "\n",
    "    intersect_ccy = list(present_ccy & known_ccy)\n",
    "    unknown_ccy = list(present_ccy - known_ccy)\n",
    "\n",
    "    parts = []\n",
    "\n",
    "    for cur in intersect_ccy:\n",
    "        sub = df[(df[\"currency\"] == cur)].copy()\n",
    "        sub_with_date = sub[sub[\"date\"].notna()].copy()\n",
    "        sub_no_date = sub[sub[\"date\"].isna()].copy()  # позже вернём\n",
    "\n",
    "        fx_cur = fx_long[(fx_long[\"currency\"] == cur) & (fx_long[\"date\"].notna())].copy()\n",
    "\n",
    "        sub_with_date.sort_values(\"date\", kind=\"mergesort\", inplace=True)\n",
    "        fx_cur.sort_values(\"date\", kind=\"mergesort\", inplace=True)\n",
    "\n",
    "        if cur == \"USD\":\n",
    "            sub_with_date[\"rate\"] = 1.0\n",
    "            sub_with_date[\"amount_usd\"] = sub_with_date[\"amount\"]\n",
    "        else:\n",
    "            if fx_cur.empty:\n",
    "                sub_with_date[\"rate\"] = np.nan\n",
    "                sub_with_date[\"amount_usd\"] = np.nan\n",
    "            else:\n",
    "                merged_cur = pd.merge_asof(\n",
    "                    sub_with_date,\n",
    "                    fx_cur[[\"date\", \"rate\"]].copy(),\n",
    "                    on=\"date\",\n",
    "                    direction=\"nearest\",\n",
    "                    tolerance=pd.Timedelta(days=tolerance_days),\n",
    "                    allow_exact_matches=True,\n",
    "                )\n",
    "                merged_cur[\"rate\"] = merged_cur[\"rate\"].replace({0: np.nan})\n",
    "                merged_cur[\"amount_usd\"] = merged_cur[\"amount\"] / merged_cur[\"rate\"]\n",
    "                sub_with_date = merged_cur\n",
    "\n",
    "        if not sub_no_date.empty:\n",
    "            if cur == \"USD\":\n",
    "                sub_no_date[\"rate\"] = 1.0\n",
    "                sub_no_date[\"amount_usd\"] = sub_no_date[\"amount\"]\n",
    "            else:\n",
    "                sub_no_date[\"rate\"] = np.nan\n",
    "                sub_no_date[\"amount_usd\"] = np.nan\n",
    "\n",
    "        parts.append(sub_with_date)\n",
    "        if not sub_no_date.empty:\n",
    "            parts.append(sub_no_date)\n",
    "\n",
    "    if unknown_ccy:\n",
    "        for cur in unknown_ccy:\n",
    "            sub = df[(df[\"currency\"] == cur)].copy()\n",
    "            if cur == \"USD\":\n",
    "                sub[\"rate\"] = 1.0\n",
    "                sub[\"amount_usd\"] = sub[\"amount\"]\n",
    "            else:\n",
    "                sub[\"rate\"] = np.nan\n",
    "                sub[\"amount_usd\"] = np.nan\n",
    "            parts.append(sub)\n",
    "\n",
    "    sub_nan_ccy = df[df[\"currency\"].isna()].copy()\n",
    "    if not sub_nan_ccy.empty:\n",
    "        sub_nan_ccy[\"rate\"] = np.nan\n",
    "        sub_nan_ccy[\"amount_usd\"] = np.where(sub_nan_ccy[\"currency\"] == \"USD\", sub_nan_ccy[\"amount\"], np.nan)\n",
    "        parts.append(sub_nan_ccy)\n",
    "\n",
    "    if parts:\n",
    "        merged_all = pd.concat(parts, ignore_index=True)\n",
    "    else:\n",
    "        merged_all = df.copy()\n",
    "        merged_all[\"rate\"] = np.nan\n",
    "        merged_all[\"amount_usd\"] = np.where(merged_all[\"currency\"] == \"USD\", merged_all[\"amount\"], np.nan)\n",
    "\n",
    "    merged_all.sort_values(\"_orig_idx\", inplace=True)\n",
    "    merged_all.drop(columns=[\"_orig_idx\"], inplace=True)\n",
    "\n",
    "    unknown_mask = ~df[\"currency\"].isin(list(known_ccy)) & df[\"currency\"].notna()\n",
    "    summary = {\n",
    "        \"total_rows\": int(len(df)),\n",
    "        \"unknown_currency_rows\": int(unknown_mask.sum()),\n",
    "        \"unknown_currency_unique\": int(df.loc[unknown_mask, \"currency\"].nunique()),\n",
    "        \"rows_without_rate_after_join\": int(merged_all[\"amount_usd\"].isna().sum()),\n",
    "    }\n",
    "\n",
    "    return merged_all, summary\n",
    "\n"
   ],
   "id": "45d4cc78a9de4108",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T21:40:01.423168Z",
     "start_time": "2025-08-11T21:40:01.411081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Метрики и сегменты\n",
    "\n",
    "def compute_core_kpis(df: pd.DataFrame) -> Dict[str, float]:\n",
    "    kpis = {}\n",
    "    kpis[\"rows\"] = int(len(df))\n",
    "    kpis[\"customers\"] = int(df[\"customer_id\"].nunique()) if \"customer_id\" in df.columns else None\n",
    "    kpis[\"vendors\"] = int(df[\"vendor\"].nunique()) if \"vendor\" in df.columns else None\n",
    "    kpis[\"countries\"] = int(df[\"country\"].nunique()) if \"country\" in df.columns else None\n",
    "    kpis[\"cities\"] = int(df[\"city\"].nunique()) if \"city\" in df.columns else None\n",
    "    kpis[\"fraud_rate_overall\"] = float(df[\"is_fraud\"].mean()) if \"is_fraud\" in df.columns else None\n",
    "    if \"amount_usd\" in df.columns:\n",
    "        col = \"amount_usd\"\n",
    "    else:\n",
    "        col = \"amount\"\n",
    "    kpis[\"median_amount\"] = float(pd.to_numeric(df[col], errors=\"coerce\").median())\n",
    "    kpis[\"p95_amount\"] = float(pd.to_numeric(df[col], errors=\"coerce\").quantile(0.95))\n",
    "    kpis[\"p99_amount\"] = float(pd.to_numeric(df[col], errors=\"coerce\").quantile(0.99))\n",
    "    return kpis\n",
    "\n",
    "def segment_tables(df: pd.DataFrame) -> Dict[str, pd.DataFrame]:\n",
    "    seg = {}\n",
    "\n",
    "    def agg_rate(g):\n",
    "        return pd.Series({\n",
    "            \"count\": len(g),\n",
    "            \"fraud_count\": int(g[\"is_fraud\"].sum()),\n",
    "            \"fraud_rate\": float(g[\"is_fraud\"].mean() if len(g) else np.nan),\n",
    "            \"avg_amount\": float(pd.to_numeric(g[\"amount_usd\"] if \"amount_usd\" in g else g[\"amount\"], errors=\"coerce\").mean())\n",
    "        })\n",
    "\n",
    "    if \"country\" in df.columns:\n",
    "        seg[\"by_country\"] = df.groupby(\"country\", dropna=False).apply(agg_rate).sort_values(\"fraud_count\", ascending=False)\n",
    "\n",
    "    if \"city\" in df.columns:\n",
    "        seg[\"by_city\"] = df.groupby(\"city\", dropna=False).apply(agg_rate).sort_values(\"avg_amount\", ascending=False)\n",
    "\n",
    "    if \"vendor_category\" in df.columns:\n",
    "        seg[\"by_vendor_category\"] = df.groupby(\"vendor_category\", dropna=False).apply(agg_rate).sort_values(\"fraud_rate\", ascending=False)\n",
    "\n",
    "    if \"device\" in df.columns:\n",
    "        seg[\"by_device\"] = df.groupby(\"device\", dropna=False).apply(agg_rate).sort_values(\"fraud_rate\", ascending=False)\n",
    "\n",
    "    if \"channel\" in df.columns:\n",
    "        seg[\"by_channel\"] = df.groupby(\"channel\", dropna=False).apply(agg_rate).sort_values(\"fraud_rate\", ascending=False)\n",
    "\n",
    "    if \"is_card_present\" in df.columns:\n",
    "        seg[\"by_card_present\"] = df.groupby(\"is_card_present\", dropna=False).apply(agg_rate).sort_values(\"fraud_rate\", ascending=False)\n",
    "\n",
    "    for col in [\"is_outside_home_country\", \"is_weekend\", \"is_high_risk_vendor\"]:\n",
    "        if col in df.columns:\n",
    "            seg[f\"by_{col}\"] = df.groupby(col, dropna=False).apply(agg_rate).sort_values(\"fraud_rate\", ascending=False)\n",
    "\n",
    "    if \"vendor\" in df.columns:\n",
    "        seg[\"top_vendors_by_fraud\"] = (\n",
    "            df.groupby(\"vendor\")[\"is_fraud\"]\n",
    "            .agg([\"count\", \"sum\"])\n",
    "            .rename(columns={\"sum\": \"fraud_count\"})\n",
    "            .sort_values(\"fraud_count\", ascending=False)\n",
    "            .head(20)\n",
    "        )\n",
    "    return seg\n",
    "\n",
    "def per_customer_hour_rate(df: pd.DataFrame) -> float:\n",
    "    tmp = df[[\"customer_id\", \"timestamp\"]].dropna().copy()\n",
    "    tmp[\"hour\"] = tmp[\"timestamp\"].dt.floor(\"H\")\n",
    "    grp = tmp.groupby([\"customer_id\", \"hour\"]).size().rename(\"tx_count\").reset_index()\n",
    "    return float(grp[\"tx_count\"].mean()) if len(grp) else float(\"nan\")\n"
   ],
   "id": "76db2aa289052eec",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T21:40:01.454467Z",
     "start_time": "2025-08-11T21:40:01.439364Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "# Стриминговый EDA: KPI/сегменты без загрузки всего файла в память\n",
    "\n",
    "def streaming_kpis_and_segments(tx_path: Path, out_dir: Path, topk: int = 20, batch_size: int = 200_000) -> tuple[Any, dict[str, Any]]:\n",
    "    pf = pq.ParquetFile(tx_path)\n",
    "    cols = [\"customer_id\",\"vendor\",\"vendor_category\",\"country\",\"city\",\"is_fraud\",\"timestamp\"]\n",
    "\n",
    "    total_rows = 0\n",
    "    fraud_count = 0\n",
    "    uniq_customers, uniq_vendors, uniq_countries, uniq_cities = set(), set(), set(), set()\n",
    "\n",
    "    by_country_count, by_country_fraud = Counter(), Counter()\n",
    "    by_vcat_count, by_vcat_fraud = Counter(), Counter()\n",
    "    by_vendor_count, by_vendor_fraud = Counter(), Counter()\n",
    "\n",
    "    per_cust_hour = Counter()\n",
    "\n",
    "    for batch in pf.iter_batches(columns=cols, batch_size=batch_size):\n",
    "        tb = pa.Table.from_batches([batch])\n",
    "        df = tb.to_pandas(types_mapper=pd.ArrowDtype)\n",
    "\n",
    "        df[\"is_fraud\"] = pd.to_numeric(df[\"is_fraud\"], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "        n = len(df)\n",
    "        total_rows += n\n",
    "        fraud_count += int(df[\"is_fraud\"].sum())\n",
    "\n",
    "        uniq_customers.update(df[\"customer_id\"].dropna().unique().tolist())\n",
    "        uniq_vendors.update(df[\"vendor\"].dropna().unique().tolist())\n",
    "        uniq_countries.update(df[\"country\"].dropna().unique().tolist())\n",
    "        uniq_cities.update(df[\"city\"].dropna().unique().tolist())\n",
    "\n",
    "        grp_c = df.groupby(\"country\")[\"is_fraud\"].agg([\"size\",\"sum\"])\n",
    "        for k, row in grp_c.iterrows():\n",
    "            by_country_count[k] += int(row[\"size\"])\n",
    "            by_country_fraud[k] += int(row[\"sum\"])\n",
    "\n",
    "        grp_vc = df.groupby(\"vendor_category\")[\"is_fraud\"].agg([\"size\",\"sum\"])\n",
    "        for k, row in grp_vc.iterrows():\n",
    "            by_vcat_count[k] += int(row[\"size\"])\n",
    "            by_vcat_fraud[k] += int(row[\"sum\"])\n",
    "\n",
    "        grp_v = df.groupby(\"vendor\")[\"is_fraud\"].agg([\"size\",\"sum\"])\n",
    "        for k, row in grp_v.iterrows():\n",
    "            by_vendor_count[k] += int(row[\"size\"])\n",
    "            by_vendor_fraud[k] += int(row[\"sum\"])\n",
    "\n",
    "        # среднее число транзакций на клиента-час\n",
    "        df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "        df[\"hour\"] = df[\"timestamp\"].dt.floor(\"H\")\n",
    "        grp_hour = df.groupby([\"customer_id\",\"hour\"]).size()\n",
    "        for key, val in grp_hour.items():\n",
    "            per_cust_hour[key] += int(val)\n",
    "\n",
    "    df_country = pd.DataFrame({\n",
    "        \"country\": list(by_country_count.keys()),\n",
    "        \"count\": [by_country_count[k] for k in by_country_count.keys()],\n",
    "        \"fraud_count\": [by_country_fraud.get(k, 0) for k in by_country_count.keys()],\n",
    "    })\n",
    "    df_country[\"fraud_rate\"] = np.where(df_country[\"count\"]>0, df_country[\"fraud_count\"]/df_country[\"count\"], np.nan)\n",
    "    df_country = df_country.sort_values(\"fraud_count\", ascending=False)\n",
    "\n",
    "    df_vcat = pd.DataFrame({\n",
    "        \"vendor_category\": list(by_vcat_count.keys()),\n",
    "        \"count\": [by_vcat_count[k] for k in by_vcat_count.keys()],\n",
    "        \"fraud_count\": [by_vcat_fraud.get(k, 0) for k in by_vcat_count.keys()],\n",
    "    })\n",
    "    df_vcat[\"fraud_rate\"] = np.where(df_vcat[\"count\"]>0, df_vcat[\"fraud_count\"]/df_vcat[\"count\"], np.nan)\n",
    "    df_vcat = df_vcat.sort_values(\"fraud_rate\", ascending=False)\n",
    "\n",
    "    df_vendor = pd.DataFrame({\n",
    "        \"vendor\": list(by_vendor_count.keys()),\n",
    "        \"count\": [by_vendor_count[k] for k in by_vendor_count.keys()],\n",
    "        \"fraud_count\": [by_vendor_fraud.get(k, 0) for k in by_vendor_count.keys()],\n",
    "    })\n",
    "    df_vendor = df_vendor.sort_values(\"fraud_count\", ascending=False).head(topk)\n",
    "\n",
    "    if len(per_cust_hour):\n",
    "        avg_tx_per_customer_hour = float(np.mean(list(per_cust_hour.values())))\n",
    "    else:\n",
    "        avg_tx_per_customer_hour = float(\"nan\")\n",
    "\n",
    "    kpis = {\n",
    "        \"rows\": int(total_rows),\n",
    "        \"customers\": int(len(uniq_customers)),\n",
    "        \"vendors\": int(len(uniq_vendors)),\n",
    "        \"countries\": int(len(uniq_countries)),\n",
    "        \"cities\": int(len(uniq_cities)),\n",
    "        \"fraud_rate_overall\": float(fraud_count / total_rows) if total_rows else None,\n",
    "        \"avg_tx_per_customer_hour\": avg_tx_per_customer_hour,\n",
    "    }\n",
    "\n",
    "    df_country.to_csv(OUT_DIR / \"seg_by_country.csv\", index=False)\n",
    "    df_vcat.to_csv(OUT_DIR / \"seg_by_vendor_category.csv\", index=False)\n",
    "    df_vendor.to_csv(OUT_DIR / \"top_vendors_by_fraud.csv\", index=False)\n",
    "    (OUT_DIR / \"kpis_streaming.json\").write_text(json.dumps(kpis, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    return kpis, {\"by_country\": df_country, \"by_vendor_category\": df_vcat, \"top_vendors_by_fraud\": df_vendor}\n"
   ],
   "id": "4b90bf7f22e38e8c",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T21:40:01.486205Z",
     "start_time": "2025-08-11T21:40:01.471094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Визуализации\n",
    "\n",
    "def plot_amount_histogram(df: pd.DataFrame, out_path: Path):\n",
    "    plt.figure()\n",
    "    if \"amount_usd\" in df.columns and df[\"amount_usd\"].notna().any():\n",
    "        vals = pd.to_numeric(df[\"amount_usd\"], errors=\"coerce\").dropna()\n",
    "        xlabel = \"Amount in USD\"\n",
    "    else:\n",
    "        vals = pd.to_numeric(df[\"amount\"], errors=\"coerce\").dropna()\n",
    "        xlabel = \"Amount (raw)\"\n",
    "    plt.hist(vals, bins=50)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Distribution of transaction amounts\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_fraud_rate_by_vendor_category(df: pd.DataFrame, out_path: Path):\n",
    "    if \"vendor_category\" not in df.columns:\n",
    "        return\n",
    "    fr = (\n",
    "        df.groupby(\"vendor_category\")[\"is_fraud\"]\n",
    "        .mean()\n",
    "        .sort_values(ascending=False)\n",
    "        .reset_index()\n",
    "    )\n",
    "    plt.figure()\n",
    "    plt.bar(fr[\"vendor_category\"].astype(str), fr[\"is_fraud\"])\n",
    "    plt.xlabel(\"Vendor category\")\n",
    "    plt.ylabel(\"Fraud rate\")\n",
    "    plt.title(\"Fraud rate by vendor category\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path)\n",
    "    plt.close()\n"
   ],
   "id": "f29f801fb2f06ce",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T21:41:12.455454Z",
     "start_time": "2025-08-11T21:40:17.332925Z"
    }
   },
   "cell_type": "code",
   "source": [
    "use_cols = None\n",
    "df_full = pd.read_parquet(TX_PATH, columns=use_cols)\n",
    "df_full = normalize_timestamp(df_full)\n",
    "df_full = expand_last_hour_activity(df_full)\n",
    "\n",
    "fx = pd.read_parquet(FX_PATH)\n",
    "df_full, fx_summary = convert_to_usd(df_full, fx)\n",
    "print(\"FX summary:\", fx_summary)\n",
    "\n",
    "kpis_mem = compute_core_kpis(df_full)\n",
    "avg_tx_per_hour = per_customer_hour_rate(df_full)\n",
    "segments = segment_tables(df_full)\n",
    "\n",
    "print(\"KPI (in-memory):\")\n",
    "display(pd.Series({**kpis_mem, \"avg_tx_per_customer_hour\": avg_tx_per_hour}))\n",
    "\n",
    "if \"by_country\" in segments:\n",
    "    print(\"\\nFraud по странам (top 10):\")\n",
    "    display(segments[\"by_country\"].head(10))\n",
    "if \"by_vendor_category\" in segments:\n",
    "    print(\"\\nFraud по категориям мерчантов (top 15 по fraud_rate):\")\n",
    "    display(segments[\"by_vendor_category\"].head(15))\n",
    "if \"top_vendors_by_fraud\" in segments:\n",
    "    print(\"\\nTop-20 мерчантов по числу мошенничеств:\")\n",
    "    display(segments[\"top_vendors_by_fraud\"])\n",
    "\n",
    "hist_path = PLOTS_DIR / \"amount_hist.png\"\n",
    "plot_amount_histogram(df_full, hist_path)\n",
    "print(\"Гистограмма сумм сохранена в:\", hist_path)\n",
    "\n",
    "fr_by_cat_path = PLOTS_DIR / \"fraud_rate_by_vendor_category.png\"\n",
    "plot_fraud_rate_by_vendor_category(df_full, fr_by_cat_path)\n",
    "print(\"График fraud_rate по категориям мерчантов сохранён в:\", fr_by_cat_path)\n"
   ],
   "id": "4bf0540fcf43ccad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FX summary: {'total_rows': 7483766, 'unknown_currency_rows': 0, 'unknown_currency_unique': 0, 'rows_without_rate_after_join': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12056\\1202726587.py:65: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  tmp[\"hour\"] = tmp[\"timestamp\"].dt.floor(\"H\")\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12056\\1202726587.py:32: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  seg[\"by_country\"] = df.groupby(\"country\", dropna=False).apply(agg_rate).sort_values(\"fraud_count\", ascending=False)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12056\\1202726587.py:35: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  seg[\"by_city\"] = df.groupby(\"city\", dropna=False).apply(agg_rate).sort_values(\"avg_amount\", ascending=False)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12056\\1202726587.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  seg[\"by_vendor_category\"] = df.groupby(\"vendor_category\", dropna=False).apply(agg_rate).sort_values(\"fraud_rate\", ascending=False)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12056\\1202726587.py:41: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  seg[\"by_device\"] = df.groupby(\"device\", dropna=False).apply(agg_rate).sort_values(\"fraud_rate\", ascending=False)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12056\\1202726587.py:44: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  seg[\"by_channel\"] = df.groupby(\"channel\", dropna=False).apply(agg_rate).sort_values(\"fraud_rate\", ascending=False)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12056\\1202726587.py:47: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  seg[\"by_card_present\"] = df.groupby(\"is_card_present\", dropna=False).apply(agg_rate).sort_values(\"fraud_rate\", ascending=False)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12056\\1202726587.py:51: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  seg[f\"by_{col}\"] = df.groupby(col, dropna=False).apply(agg_rate).sort_values(\"fraud_rate\", ascending=False)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12056\\1202726587.py:51: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  seg[f\"by_{col}\"] = df.groupby(col, dropna=False).apply(agg_rate).sort_values(\"fraud_rate\", ascending=False)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12056\\1202726587.py:51: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  seg[f\"by_{col}\"] = df.groupby(col, dropna=False).apply(agg_rate).sort_values(\"fraud_rate\", ascending=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KPI (in-memory):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rows                        7.483766e+06\n",
       "customers                   4.869000e+03\n",
       "vendors                     1.050000e+02\n",
       "countries                   1.200000e+01\n",
       "cities                      1.100000e+01\n",
       "fraud_rate_overall          1.997282e-01\n",
       "median_amount               3.592600e+02\n",
       "p95_amount                  1.735918e+03\n",
       "p99_amount                  2.885956e+03\n",
       "avg_tx_per_customer_hour    2.452776e+00\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fraud по странам (top 10):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "              count  fraud_count  fraud_rate  avg_amount\n",
       "country                                                 \n",
       "Russia     793730.0     299425.0    0.377238  590.245348\n",
       "Mexico     785704.0     298841.0    0.380348  772.656684\n",
       "Brazil     804800.0     298629.0    0.371060  676.081344\n",
       "Nigeria    849840.0     298600.0    0.351360  187.987569\n",
       "Australia  496695.0      37652.0    0.075805  532.699657\n",
       "Japan      527393.0      37592.0    0.071279  437.522896\n",
       "France     541287.0      37426.0    0.069143  529.359772\n",
       "Singapore  588668.0      37414.0    0.063557  616.041699\n",
       "UK         538493.0      37345.0    0.069351  550.406134\n",
       "USA        500060.0      37312.0    0.074615  565.539111"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>fraud_count</th>\n",
       "      <th>fraud_rate</th>\n",
       "      <th>avg_amount</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>country</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Russia</th>\n",
       "      <td>793730.0</td>\n",
       "      <td>299425.0</td>\n",
       "      <td>0.377238</td>\n",
       "      <td>590.245348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mexico</th>\n",
       "      <td>785704.0</td>\n",
       "      <td>298841.0</td>\n",
       "      <td>0.380348</td>\n",
       "      <td>772.656684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Brazil</th>\n",
       "      <td>804800.0</td>\n",
       "      <td>298629.0</td>\n",
       "      <td>0.371060</td>\n",
       "      <td>676.081344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nigeria</th>\n",
       "      <td>849840.0</td>\n",
       "      <td>298600.0</td>\n",
       "      <td>0.351360</td>\n",
       "      <td>187.987569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Australia</th>\n",
       "      <td>496695.0</td>\n",
       "      <td>37652.0</td>\n",
       "      <td>0.075805</td>\n",
       "      <td>532.699657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Japan</th>\n",
       "      <td>527393.0</td>\n",
       "      <td>37592.0</td>\n",
       "      <td>0.071279</td>\n",
       "      <td>437.522896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>France</th>\n",
       "      <td>541287.0</td>\n",
       "      <td>37426.0</td>\n",
       "      <td>0.069143</td>\n",
       "      <td>529.359772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Singapore</th>\n",
       "      <td>588668.0</td>\n",
       "      <td>37414.0</td>\n",
       "      <td>0.063557</td>\n",
       "      <td>616.041699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UK</th>\n",
       "      <td>538493.0</td>\n",
       "      <td>37345.0</td>\n",
       "      <td>0.069351</td>\n",
       "      <td>550.406134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USA</th>\n",
       "      <td>500060.0</td>\n",
       "      <td>37312.0</td>\n",
       "      <td>0.074615</td>\n",
       "      <td>565.539111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fraud по категориям мерчантов (top 15 по fraud_rate):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                    count  fraud_count  fraud_rate   avg_amount\n",
       "vendor_category                                                \n",
       "Travel           935790.0     187477.0    0.200341  1040.826006\n",
       "Grocery          934029.0     186987.0    0.200194   416.534478\n",
       "Gas              935401.0     186829.0    0.199731   513.259443\n",
       "Restaurant       936178.0     186951.0    0.199696   323.328116\n",
       "Entertainment    936173.0     186890.0    0.199632   350.314504\n",
       "Education        933542.0     186203.0    0.199459   514.491554\n",
       "Retail           935883.0     186613.0    0.199398   667.305228\n",
       "Healthcare       936770.0     186769.0    0.199376   514.887331"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>fraud_count</th>\n",
       "      <th>fraud_rate</th>\n",
       "      <th>avg_amount</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vendor_category</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Travel</th>\n",
       "      <td>935790.0</td>\n",
       "      <td>187477.0</td>\n",
       "      <td>0.200341</td>\n",
       "      <td>1040.826006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Grocery</th>\n",
       "      <td>934029.0</td>\n",
       "      <td>186987.0</td>\n",
       "      <td>0.200194</td>\n",
       "      <td>416.534478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gas</th>\n",
       "      <td>935401.0</td>\n",
       "      <td>186829.0</td>\n",
       "      <td>0.199731</td>\n",
       "      <td>513.259443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Restaurant</th>\n",
       "      <td>936178.0</td>\n",
       "      <td>186951.0</td>\n",
       "      <td>0.199696</td>\n",
       "      <td>323.328116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Entertainment</th>\n",
       "      <td>936173.0</td>\n",
       "      <td>186890.0</td>\n",
       "      <td>0.199632</td>\n",
       "      <td>350.314504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Education</th>\n",
       "      <td>933542.0</td>\n",
       "      <td>186203.0</td>\n",
       "      <td>0.199459</td>\n",
       "      <td>514.491554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Retail</th>\n",
       "      <td>935883.0</td>\n",
       "      <td>186613.0</td>\n",
       "      <td>0.199398</td>\n",
       "      <td>667.305228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Healthcare</th>\n",
       "      <td>936770.0</td>\n",
       "      <td>186769.0</td>\n",
       "      <td>0.199376</td>\n",
       "      <td>514.887331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top-20 мерчантов по числу мошенничеств:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                       count  fraud_count\n",
       "vendor                                   \n",
       "Local Gas Station     155977        31205\n",
       "Truck Stop            155945        31192\n",
       "Chegg                 156105        31171\n",
       "University Bookstore  155827        31136\n",
       "Highway Gas Stop      155980        31016\n",
       "Barnes & Noble        154833        30933\n",
       "DuaneReade            116938        23513\n",
       "CVS Pharmacy          117377        23508\n",
       "Walmart Grocery       116349        23475\n",
       "Medical Center        117149        23464\n",
       "Amazon Fresh          116774        23427\n",
       "Instacart             116996        23360\n",
       "Lab Corp              117124        23351\n",
       "Rite Aid              117242        23305\n",
       "Walgreens             116820        23243\n",
       "Urgent Care           117139        23215\n",
       "Local Hospital        116981        23170\n",
       "FreshDirect           117112        23134\n",
       "Skillshare             93285        18663\n",
       "Coursera               93261        18658"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>fraud_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vendor</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Local Gas Station</th>\n",
       "      <td>155977</td>\n",
       "      <td>31205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Truck Stop</th>\n",
       "      <td>155945</td>\n",
       "      <td>31192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chegg</th>\n",
       "      <td>156105</td>\n",
       "      <td>31171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>University Bookstore</th>\n",
       "      <td>155827</td>\n",
       "      <td>31136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Highway Gas Stop</th>\n",
       "      <td>155980</td>\n",
       "      <td>31016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Barnes &amp; Noble</th>\n",
       "      <td>154833</td>\n",
       "      <td>30933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DuaneReade</th>\n",
       "      <td>116938</td>\n",
       "      <td>23513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CVS Pharmacy</th>\n",
       "      <td>117377</td>\n",
       "      <td>23508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Walmart Grocery</th>\n",
       "      <td>116349</td>\n",
       "      <td>23475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Medical Center</th>\n",
       "      <td>117149</td>\n",
       "      <td>23464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amazon Fresh</th>\n",
       "      <td>116774</td>\n",
       "      <td>23427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Instacart</th>\n",
       "      <td>116996</td>\n",
       "      <td>23360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lab Corp</th>\n",
       "      <td>117124</td>\n",
       "      <td>23351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rite Aid</th>\n",
       "      <td>117242</td>\n",
       "      <td>23305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Walgreens</th>\n",
       "      <td>116820</td>\n",
       "      <td>23243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Urgent Care</th>\n",
       "      <td>117139</td>\n",
       "      <td>23215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Local Hospital</th>\n",
       "      <td>116981</td>\n",
       "      <td>23170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FreshDirect</th>\n",
       "      <td>117112</td>\n",
       "      <td>23134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Skillshare</th>\n",
       "      <td>93285</td>\n",
       "      <td>18663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Coursera</th>\n",
       "      <td>93261</td>\n",
       "      <td>18658</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Гистограмма сумм сохранена в: eda_outputs\\plots\\amount_hist.png\n",
      "График fraud_rate по категориям мерчантов сохранён в: eda_outputs\\plots\\fraud_rate_by_vendor_category.png\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T21:41:12.910802Z",
     "start_time": "2025-08-11T21:41:12.472541Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Метрики: threshold под целевой FPR и Expected Cost\n",
    "\n",
    "from typing import Tuple, Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, average_precision_score, roc_auc_score\n",
    "\n",
    "def threshold_at_fpr(y_true: np.ndarray, y_score: np.ndarray, target_fpr: float = 0.005) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Возвращает порог, при котором FPR <= target_fpr (0.5% по умолчанию),\n",
    "    а Recall (TPR) максимален. Если таких точек нет — берёт ближайшую сверху.\n",
    "    \"\"\"\n",
    "    fpr, tpr, thr = roc_curve(y_true, y_score)\n",
    "    ok = np.where(fpr <= target_fpr)[0]\n",
    "    if len(ok) > 0:\n",
    "        i = ok[np.argmax(tpr[ok])]\n",
    "    else:\n",
    "        # нет ни одной точки с FPR ниже порога — берём минимальный FPR\n",
    "        i = np.argmin(fpr)\n",
    "    return {\n",
    "        \"threshold\": float(thr[i]),\n",
    "        \"fpr\": float(fpr[i]),\n",
    "        \"tpr_recall\": float(tpr[i]),\n",
    "    }\n",
    "\n",
    "def confusion_at_threshold(y_true: np.ndarray, y_score: np.ndarray, thr: float) -> Dict[str, int]:\n",
    "    y_pred = (y_score >= thr).astype(int)\n",
    "    tp = int(((y_true == 1) & (y_pred == 1)).sum())\n",
    "    fp = int(((y_true == 0) & (y_pred == 1)).sum())\n",
    "    fn = int(((y_true == 1) & (y_pred == 0)).sum())\n",
    "    tn = int(((y_true == 0) & (y_pred == 0)).sum())\n",
    "    return {\"TP\": tp, \"FP\": fp, \"FN\": fn, \"TN\": tn}\n",
    "\n",
    "def expected_cost(y_true: np.ndarray, y_score: np.ndarray, thr: float, c_fp: float, c_fn: float) -> Dict[str, float]:\n",
    "    cm = confusion_at_threshold(y_true, y_score, thr)\n",
    "    cost = cm[\"FP\"] * c_fp + cm[\"FN\"] * c_fn\n",
    "    return {\n",
    "        \"expected_cost\": float(cost),\n",
    "        \"TP\": cm[\"TP\"], \"FP\": cm[\"FP\"], \"FN\": cm[\"FN\"], \"TN\": cm[\"TN\"]\n",
    "    }\n",
    "\n",
    "def compute_all_core_metrics(y_true: np.ndarray, y_score: np.ndarray) -> Dict[str, float]:\n",
    "    roc = roc_auc_score(y_true, y_score)\n",
    "    pr = average_precision_score(y_true, y_score)  # PR AUC (AP)\n",
    "    return {\"roc_auc\": float(roc), \"pr_auc\": float(pr)}\n"
   ],
   "id": "d155f9fae38f5f3f",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T21:41:12.941634Z",
     "start_time": "2025-08-11T21:41:12.927014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def _bar_fraud_rate(df: pd.DataFrame, col: str, out_path: Path):\n",
    "    if col not in df.columns:\n",
    "        return\n",
    "    tmp = df.copy()\n",
    "    grp = tmp.groupby(col)[\"is_fraud\"].mean().sort_values(ascending=False)\n",
    "    plt.figure()\n",
    "    plt.bar(grp.index.astype(str), grp.values)\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Fraud rate\")\n",
    "    plt.title(f\"Fraud rate by {col}\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_fraud_by_channel_device(df: pd.DataFrame):\n",
    "    _bar_fraud_rate(df, \"channel\", PLOTS_DIR / \"fraud_rate_by_channel.png\")\n",
    "    _bar_fraud_rate(df, \"device\", PLOTS_DIR / \"fraud_rate_by_device.png\")\n",
    "\n",
    "def plot_fraud_by_hour_day(df: pd.DataFrame):\n",
    "    if \"timestamp\" not in df.columns:\n",
    "        return\n",
    "    tmp = df.copy()\n",
    "    tmp[\"hour\"] = tmp[\"timestamp\"].dt.hour\n",
    "    tmp[\"dow\"] = tmp[\"timestamp\"].dt.dayofweek  # 0=Mon .. 6=Sun\n",
    "\n",
    "    # Hour-of-day\n",
    "    fr_hour = tmp.groupby(\"hour\")[\"is_fraud\"].mean()\n",
    "    plt.figure()\n",
    "    plt.plot(fr_hour.index.values, fr_hour.values, marker=\"o\")\n",
    "    plt.xlabel(\"Hour of day\")\n",
    "    plt.ylabel(\"Fraud rate\")\n",
    "    plt.title(\"Fraud rate by hour of day\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PLOTS_DIR / \"fraud_rate_by_hour.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Day-of-week\n",
    "    fr_dow = tmp.groupby(\"dow\")[\"is_fraud\"].mean()\n",
    "    plt.figure()\n",
    "    plt.plot(fr_dow.index.values, fr_dow.values, marker=\"o\")\n",
    "    plt.xlabel(\"Day of week (0=Mon)\")\n",
    "    plt.ylabel(\"Fraud rate\")\n",
    "    plt.title(\"Fraud rate by day of week\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PLOTS_DIR / \"fraud_rate_by_dow.png\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_daily_timeseries(df: pd.DataFrame):\n",
    "    if \"timestamp\" not in df.columns:\n",
    "        return\n",
    "    tmp = df.copy()\n",
    "    tmp[\"day\"] = tmp[\"timestamp\"].dt.normalize()\n",
    "    series = tmp.groupby(\"day\")[\"is_fraud\"].mean()\n",
    "    plt.figure()\n",
    "    plt.plot(series.index.values, series.values)\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Fraud rate\")\n",
    "    plt.title(\"Daily fraud rate\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PLOTS_DIR / \"daily_fraud_rate.png\")\n",
    "    plt.close()\n"
   ],
   "id": "38c7c19f003592b",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T21:45:05.738514Z",
     "start_time": "2025-08-11T21:45:05.724748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def build_feature_space(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Гибко собираем списки признаков на основе наличия колонок.\n",
    "    - numeric: amount_usd (если есть) или amount, lha_* фичи, бинарные флаги как 0/1\n",
    "    - categorical: vendor_category, channel, device, country, city (если уместно)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    for b in [\"is_card_present\", \"is_outside_home_country\", \"is_weekend\", \"is_high_risk_vendor\"]:\n",
    "        if b in df.columns:\n",
    "            df[b] = df[b].astype(float)  # приведём к числу (0/1/NaN)\n",
    "\n",
    "    numeric = []\n",
    "    if \"amount_usd\" in df.columns:\n",
    "        numeric.append(\"amount_usd\")\n",
    "    elif \"amount\" in df.columns:\n",
    "        numeric.append(\"amount\")\n",
    "    # lha_* (из last_hour_activity)\n",
    "    lha_cols = [c for c in df.columns if c.startswith(\"lha_\")]\n",
    "    numeric += lha_cols\n",
    "    for b in [\"is_card_present\", \"is_outside_home_country\", \"is_weekend\", \"is_high_risk_vendor\"]:\n",
    "        if b in df.columns:\n",
    "            numeric.append(b)\n",
    "\n",
    "    categorical = []\n",
    "    for c in [\"vendor_category\", \"channel\", \"device\", \"country\", \"city\"]:\n",
    "        if c in df.columns:\n",
    "            categorical.append(c)\n",
    "\n",
    "    drop_cols = {\"is_fraud\", \"timestamp\", \"date\", \"transaction_id\", \"_orig_idx\"}\n",
    "    feature_cols = [c for c in df.columns if c in (numeric + categorical) and c not in drop_cols]\n",
    "\n",
    "    numeric_step = SimpleImputer(strategy=\"median\")\n",
    "    cat_step = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_step, [c for c in feature_cols if c in numeric]),\n",
    "            (\"cat\", cat_step, [c for c in feature_cols if c in categorical]),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "        sparse_threshold=1.0,\n",
    "    )\n",
    "\n",
    "    return feature_cols, numeric, categorical, preprocessor\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "def time_block_split_no_leak_smart(\n",
    "    df: pd.DataFrame,\n",
    "    target_val_frac_customers: float = 0.20,   # хотим ~20% \"новых\" клиентов в валидации\n",
    "    min_val_rows: int = 50_000,                # минимальный размер вал-части\n",
    "    min_val_positives: int = 50,               # минимум мошеннических транзакций в вал\n",
    "    max_val_frac_customers: float = 0.50,      # верхняя граница расширения валидации\n",
    "    step_frac: float = 0.05,                   # шаг расширения, если мало данных\n",
    "    fallback_last_days: int = 14\n",
    "):\n",
    "    df = df.copy()\n",
    "    if \"timestamp\" not in df.columns:\n",
    "        raise ValueError(\"Нужен столбец 'timestamp' (datetime).\")\n",
    "    if \"customer_id\" not in df.columns:\n",
    "        raise ValueError(\"Нужен столбец 'customer_id'.\")\n",
    "    if \"is_fraud\" not in df.columns:\n",
    "        raise ValueError(\"Нужен столбец 'is_fraud'.\")\n",
    "\n",
    "    cust_first = df.groupby(\"customer_id\")[\"timestamp\"].min().sort_values()\n",
    "    unique_customers = cust_first.index.to_numpy()\n",
    "    n_cust = len(unique_customers)\n",
    "    if n_cust == 0:\n",
    "        raise ValueError(\"Нет клиентов для разреза.\")\n",
    "\n",
    "    def _split_by_fraction(frac: float):\n",
    "        k = max(1, int(round(n_cust * frac)))\n",
    "        val_customers = set(unique_customers[-k:])\n",
    "        train_customers = set(unique_customers[:-k])\n",
    "        train_df = df[df[\"customer_id\"].isin(train_customers)].copy()\n",
    "        val_df = df[df[\"customer_id\"].isin(val_customers)].copy()\n",
    "        train_df.sort_values(\"timestamp\", inplace=True)\n",
    "        val_df.sort_values(\"timestamp\", inplace=True)\n",
    "        return train_df, val_df, float(frac)\n",
    "\n",
    "    current_frac = target_val_frac_customers\n",
    "    best = None\n",
    "    while current_frac <= max_val_frac_customers + 1e-9:\n",
    "        tr, va, used_frac = _split_by_fraction(current_frac)\n",
    "        n_val = len(va)\n",
    "        n_pos = int(va[\"is_fraud\"].sum()) if n_val else 0\n",
    "        if n_val >= min_val_rows and n_pos >= min_val_positives:\n",
    "            best = (tr, va, used_frac, \"customer-first_seen\")\n",
    "            break\n",
    "        if best is None or (n_val > len(best[1])) or (n_pos > int(best[1][\"is_fraud\"].sum())):\n",
    "            best = (tr, va, used_frac, \"customer-first_seen\")\n",
    "        current_frac += step_frac\n",
    "\n",
    "    train_df, val_df, used_frac, strategy = best\n",
    "\n",
    "    if len(val_df) == 0 or val_df[\"is_fraud\"].sum() < min_val_positives:\n",
    "        max_ts = df[\"timestamp\"].max()\n",
    "        cutoff = max_ts - timedelta(days=fallback_last_days)\n",
    "        val_df_fb = df[df[\"timestamp\"] >= cutoff].copy()\n",
    "        train_df_fb = df[df[\"timestamp\"] < cutoff].copy()\n",
    "\n",
    "        seen_train_customers = set(train_df_fb[\"customer_id\"].unique())\n",
    "        val_df_fb = val_df_fb[~val_df_fb[\"customer_id\"].isin(seen_train_customers)].copy()\n",
    "\n",
    "        if len(val_df_fb) > 0 and val_df_fb[\"is_fraud\"].sum() >= min_val_positives:\n",
    "            train_df, val_df = train_df_fb, val_df_fb\n",
    "            strategy = f\"time-window-last-{fallback_last_days}d (no-leak)\"\n",
    "    print(f\"[split] Strategy: {strategy}, customers_val_frac≈{used_frac:.2f}\")\n",
    "    print(f\"[split] Train rows: {len(train_df):,}, Val rows: {len(val_df):,}, Val positives: {int(val_df['is_fraud'].sum()):,}\")\n",
    "\n",
    "    return train_df, val_df\n"
   ],
   "id": "c0f6d2a5c3674473",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T23:30:11.450545Z",
     "start_time": "2025-08-11T21:46:36.805789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Обучим 2 модели: LogisticRegression (One-Hot, sparse) и HistGradientBoosting (числовые фичи)\n",
    "# Посчитаем ROC AUC, PR AUC, Recall@FPR=0.5%, Expected Cost и построим ROC/PR графики для лучшей модели.\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import RocCurveDisplay, PrecisionRecallDisplay\n",
    "\n",
    "C_FP = 1.0    # стоимость FP\n",
    "C_FN = 20.0   # стоимость FN (примерно)\n",
    "TARGET_FPR = 0.005  # 0.5%\n",
    "\n",
    "train_df, val_df = time_block_split_no_leak_smart(df_full)\n",
    "print(\"Train rows:\", len(train_df), \"Val rows:\", len(val_df))\n",
    "\n",
    "feature_cols, numeric_cols, cat_cols, preproc = build_feature_space(train_df)\n",
    "X_train = preproc.fit_transform(train_df[feature_cols])\n",
    "y_train = train_df[\"is_fraud\"].astype(int).to_numpy()\n",
    "\n",
    "X_val = preproc.transform(val_df[feature_cols])\n",
    "y_val = val_df[\"is_fraud\"].astype(int).to_numpy()\n",
    "\n",
    "results = []\n",
    "\n",
    "logreg = LogisticRegression(\n",
    "    solver=\"saga\",\n",
    "    penalty=\"l2\",\n",
    "    class_weight=\"balanced\",\n",
    "    max_iter=2000,\n",
    "    n_jobs=None,\n",
    "    verbose=0,\n",
    ")\n",
    "pipe_lr = Pipeline([(\"model\", logreg)])\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "proba_lr = pipe_lr.predict_proba(X_val)[:, 1]\n",
    "\n",
    "m_lr = compute_all_core_metrics(y_val, proba_lr)\n",
    "thr_lr = threshold_at_fpr(y_val, proba_lr, TARGET_FPR)\n",
    "ec_lr = expected_cost(y_val, proba_lr, thr_lr[\"threshold\"], C_FP, C_FN)\n",
    "\n",
    "results.append({\n",
    "    \"model\": \"LogisticRegression\",\n",
    "    \"roc_auc\": m_lr[\"roc_auc\"],\n",
    "    \"pr_auc\": m_lr[\"pr_auc\"],\n",
    "    \"thr_at_fpr\": thr_lr[\"threshold\"],\n",
    "    \"fpr@thr\": thr_lr[\"fpr\"],\n",
    "    \"recall@thr\": thr_lr[\"tpr_recall\"],\n",
    "    \"expected_cost\": ec_lr[\"expected_cost\"],\n",
    "    \"TP\": ec_lr[\"TP\"], \"FP\": ec_lr[\"FP\"], \"FN\": ec_lr[\"FN\"], \"TN\": ec_lr[\"TN\"]\n",
    "})\n",
    "\n",
    "num_only_cols = [c for c in numeric_cols if c in feature_cols]\n",
    "if len(num_only_cols) == 0:\n",
    "    print(\"Предупреждение: нет числовых признаков для HGB — пропускаю модель.\")\n",
    "    proba_hgb = None\n",
    "else:\n",
    "    num_imputer = SimpleImputer(strategy=\"median\")\n",
    "    X_train_hgb = num_imputer.fit_transform(train_df[num_only_cols])\n",
    "    X_val_hgb = num_imputer.transform(val_df[num_only_cols])\n",
    "\n",
    "    hgb = HistGradientBoostingClassifier(\n",
    "        max_depth=None,\n",
    "        learning_rate=0.1,\n",
    "        max_iter=300,\n",
    "        l2_regularization=0.0,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1,\n",
    "        random_state=42,\n",
    "    )\n",
    "    hgb.fit(X_train_hgb, y_train)\n",
    "    proba_hgb = hgb.predict_proba(X_val_hgb)[:, 1]\n",
    "\n",
    "    m_hgb = compute_all_core_metrics(y_val, proba_hgb)\n",
    "    thr_hgb = threshold_at_fpr(y_val, proba_hgb, TARGET_FPR)\n",
    "    ec_hgb = expected_cost(y_val, proba_hgb, thr_hgb[\"threshold\"], C_FP, C_FN)\n",
    "\n",
    "    results.append({\n",
    "        \"model\": \"HistGradientBoosting(num-only)\",\n",
    "        \"roc_auc\": m_hgb[\"roc_auc\"],\n",
    "        \"pr_auc\": m_hgb[\"pr_auc\"],\n",
    "        \"thr_at_fpr\": thr_hgb[\"threshold\"],\n",
    "        \"fpr@thr\": thr_hgb[\"fpr\"],\n",
    "        \"recall@thr\": thr_hgb[\"tpr_recall\"],\n",
    "        \"expected_cost\": ec_hgb[\"expected_cost\"],\n",
    "        \"TP\": ec_hgb[\"TP\"], \"FP\": ec_hgb[\"FP\"], \"FN\": ec_hgb[\"FN\"], \"TN\": ec_hgb[\"TN\"]\n",
    "    })\n",
    "\n",
    "res_df = pd.DataFrame(results).sort_values([\"pr_auc\",\"roc_auc\"], ascending=False)\n",
    "display(res_df)\n",
    "\n",
    "best_row = res_df.iloc[0]\n",
    "best_name = best_row[\"model\"]\n",
    "\n",
    "if best_name == \"LogisticRegression\":\n",
    "    y_score_best = proba_lr\n",
    "elif best_name == \"HistGradientBoosting(num-only)\":\n",
    "    y_score_best = proba_hgb\n",
    "else:\n",
    "    y_score_best = proba_lr\n",
    "\n",
    "# ROC\n",
    "plt.figure()\n",
    "RocCurveDisplay.from_predictions(y_val, y_score_best)\n",
    "plt.title(f\"ROC curve — {best_name}\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOTS_DIR / f\"roc_{best_name.replace(' ','_')}.png\")\n",
    "plt.close()\n",
    "\n",
    "# PR\n",
    "plt.figure()\n",
    "PrecisionRecallDisplay.from_predictions(y_val, y_score_best)\n",
    "plt.title(f\"PR curve — {best_name}\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOTS_DIR / f\"pr_{best_name.replace(' ','_')}.png\")\n",
    "plt.close()\n",
    "\n",
    "# Графики по каналам/устройствам/времени для df_full (после конверсии/обогащения)\n",
    "plot_fraud_by_channel_device(df_full)\n",
    "plot_fraud_by_hour_day(df_full)\n",
    "plot_daily_timeseries(df_full)\n",
    "\n",
    "print(\"Графики сохранены в:\", PLOTS_DIR)\n"
   ],
   "id": "e8dc96d4c5fb8fbf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[split] Strategy: customer-first_seen, customers_val_frac≈0.20\n",
      "[split] Train rows: 6,108,639, Val rows: 1,375,127, Val positives: 274,636\n",
      "Train rows: 6108639 Val rows: 1375127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\PycharmProjects\\eda_finance\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                            model   roc_auc    pr_auc  thr_at_fpr   fpr@thr  \\\n",
       "1  HistGradientBoosting(num-only)  0.978166  0.942687    0.658568  0.005000   \n",
       "0              LogisticRegression  0.543659  0.226259    0.509310  0.004986   \n",
       "\n",
       "   recall@thr  expected_cost      TP    FP      FN       TN  \n",
       "1    0.741891      1423222.0  203750  5502   70886  1094989  \n",
       "0    0.011004      5437767.0    3022  5487  271614  1095004  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>pr_auc</th>\n",
       "      <th>thr_at_fpr</th>\n",
       "      <th>fpr@thr</th>\n",
       "      <th>recall@thr</th>\n",
       "      <th>expected_cost</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HistGradientBoosting(num-only)</td>\n",
       "      <td>0.978166</td>\n",
       "      <td>0.942687</td>\n",
       "      <td>0.658568</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.741891</td>\n",
       "      <td>1423222.0</td>\n",
       "      <td>203750</td>\n",
       "      <td>5502</td>\n",
       "      <td>70886</td>\n",
       "      <td>1094989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.543659</td>\n",
       "      <td>0.226259</td>\n",
       "      <td>0.509310</td>\n",
       "      <td>0.004986</td>\n",
       "      <td>0.011004</td>\n",
       "      <td>5437767.0</td>\n",
       "      <td>3022</td>\n",
       "      <td>5487</td>\n",
       "      <td>271614</td>\n",
       "      <td>1095004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Графики сохранены в: eda_outputs\\plots\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9b8140fbbc0281cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9215a34ab8581f4c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
